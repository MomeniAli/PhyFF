PhyFF: Backpropagation-free Training of Deep Physical Neural Networks
=================
[![made-with-python](https://img.shields.io/badge/Made%20with-Python-red.svg)](#python)
[![arxiv](https://img.shields.io/badge/arXiv-2211.01482-b31b1b.svg)](https://arxiv.org/abs/2304.11042)
[![PyPI version bert-score](https://badge.fury.io/py/rquge.svg)](https://pypi.python.org/pypi/rquge/)[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) 
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) 


<p align="center">
  <img src="meta.jpeg" width="500"/>
</p>

We proposed a simple deep neural network architecture augmented by a biologically plausible local learning algorithm, enabling supervised and unsupervised training of deep physical neural networks (PNNs) without requiring detailed knowledge of the nonlinear physical layersâ€™ properties. To showcase the universality of our approach, we trained diverse wave-based physical neural networks varying in underlying wave phenomenon and non-linearity to perform vowel and image classification experimentally. 


<a name="citation"/>  

Citation
-------------

<a name="citations"/>  

If you use this code for your research, please cite the following work:
```
@article{momeni2023backpropagation,
  title={Backpropagation-free Training of Deep Physical Neural Networks},
  author={Momeni, Ali and Rahmani, Babak and Mallejac, Matthieu and Del Hougne, Philipp and Fleury, Romain},
  journal={arXiv preprint arXiv:2304.11042},
  year={2023}
}
```
Have a question not listed here? 
send us an [email](ali.momeni@epfl.ch).
